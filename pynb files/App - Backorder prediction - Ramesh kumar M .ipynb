{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46fab679",
   "metadata": {},
   "source": [
    "# Predicting Material Backorder in Inventory Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a899f",
   "metadata": {},
   "source": [
    "## App using Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43a520de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07196565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants calculated from eda & feature engineering\n",
    "lead_time_mean = float(np.load('lead_time_mean.npy'))\n",
    "potential_issue_probability_matrix = pd.read_csv('potential_issue_probability_matrix.csv')\n",
    "deck_risk_probability_matrix = pd.read_csv('deck_risk_probability_matrix.csv')\n",
    "oe_constraint_probability_matrix = pd.read_csv('oe_constraint_probability_matrix.csv')\n",
    "ppap_risk_probability_matrix = pd.read_csv('ppap_risk_probability_matrix.csv')\n",
    "stop_auto_buy_probability_matrix = pd.read_csv('stop_auto_buy_probability_matrix.csv')\n",
    "rev_stop_probability_matrix = pd.read_csv('rev_stop_probability_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57b74003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):   \n",
    "    if type(x) == dict:\n",
    "        dataframe = pd.DataFrame(x, index=[0], columns=['sku', 'national_inv', 'lead_time', 'in_transit_qty',\n",
    "                                                    'forecast_3_month', 'forecast_6_month', 'forecast_9_month',\n",
    "                                                    'sales_1_month', 'sales_3_month', 'sales_6_month', 'sales_9_month',\n",
    "                                                    'min_bank', 'potential_issue', 'pieces_past_due', 'perf_6_month_avg',\n",
    "                                                    'perf_12_month_avg', 'local_bo_qty', 'deck_risk', 'oe_constraint',\n",
    "                                                    'ppap_risk', 'stop_auto_buy', 'rev_stop'])\n",
    "    else:\n",
    "        dataframe = x\n",
    "    \n",
    "    dataframe = dataframe.drop('sku', axis=1) #dropping sku column\n",
    "    \n",
    "    if dataframe.iloc[-1].isna().all() == True:\n",
    "        dataframe = dataframe[:-1] #removing last row as there are NaN values\n",
    "    \n",
    "    dataframe = dataframe.fillna(lead_time_mean) #mean imputation\n",
    "    dataframe.replace({'Yes': 1, 'No': 0}, inplace=True) #converting categorical features into binary features\n",
    "    \n",
    "    #adding binary_pieces_past_due\n",
    "    conditions = [dataframe['pieces_past_due'] == 0, dataframe['pieces_past_due'] > 0]\n",
    "    values = [0, 1]\n",
    "    dataframe['binary_pieces_past_due'] = np.select(conditions, values)\n",
    "    \n",
    "    #adding binary_local_bo_qty\n",
    "    conditions = [dataframe['local_bo_qty'] == 0, dataframe['local_bo_qty'] > 0]\n",
    "    values = [0, 1]\n",
    "    dataframe['binary_local_bo_qty'] = np.select(conditions, values)\n",
    "    \n",
    "    #imputing all categorical features\n",
    "    conditions_pt = [dataframe['potential_issue'] == 0, dataframe['potential_issue'] == 1]\n",
    "    values_pt = [potential_issue_probability_matrix['No'][0], potential_issue_probability_matrix['No'][1]]\n",
    "    dataframe['potential_issue'] = np.select(conditions_pt, values_pt)\n",
    "\n",
    "    conditions_dr = [dataframe['deck_risk'] == 0, dataframe['deck_risk'] == 1]\n",
    "    values_dr = [deck_risk_probability_matrix['No'][0], deck_risk_probability_matrix['No'][1]]\n",
    "    dataframe['deck_risk'] = np.select(conditions_dr, values_dr)\n",
    "\n",
    "    conditions_oe = [dataframe['oe_constraint'] == 0, dataframe['oe_constraint'] == 1]\n",
    "    values_oe = [oe_constraint_probability_matrix['No'][0], oe_constraint_probability_matrix['No'][1]]\n",
    "    dataframe['oe_constraint'] = np.select(conditions_oe, values_oe)\n",
    "\n",
    "    conditions_pp = [dataframe['ppap_risk'] == 0, dataframe['ppap_risk'] == 1]\n",
    "    values_pp = [ppap_risk_probability_matrix['No'][0], ppap_risk_probability_matrix['No'][1]]\n",
    "    dataframe['ppap_risk'] = np.select(conditions_pp, values_pp)\n",
    "\n",
    "    conditions_stp = [dataframe['stop_auto_buy'] == 0, dataframe['stop_auto_buy'] == 1]\n",
    "    values_stp = [stop_auto_buy_probability_matrix['No'][0], stop_auto_buy_probability_matrix['No'][1]]\n",
    "    dataframe['stop_auto_buy'] = np.select(conditions_stp, values_stp)\n",
    "\n",
    "    conditions_rev = [dataframe['rev_stop'] == 0, dataframe['rev_stop'] == 1]\n",
    "    values_rev = [rev_stop_probability_matrix['No'][0], rev_stop_probability_matrix['No'][1]]\n",
    "    dataframe['rev_stop'] = np.select(conditions_rev, values_rev)\n",
    "\n",
    "    filename = 'best_model_forest.h5'\n",
    "    best_model = pickle.load(open(filename, 'rb'))\n",
    "    predictions = best_model.predict(dataframe)\n",
    "    if len(predictions) == 1:\n",
    "        predictions = int(predictions)\n",
    "    return predictions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ca9bea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "#constants calculated from eda & feature engineering\n",
    "lead_time_mean = float(np.load('lead_time_mean.npy'))\n",
    "potential_issue_probability_matrix = pd.read_csv('potential_issue_probability_matrix.csv')\n",
    "deck_risk_probability_matrix = pd.read_csv('deck_risk_probability_matrix.csv')\n",
    "oe_constraint_probability_matrix = pd.read_csv('oe_constraint_probability_matrix.csv')\n",
    "ppap_risk_probability_matrix = pd.read_csv('ppap_risk_probability_matrix.csv')\n",
    "stop_auto_buy_probability_matrix = pd.read_csv('stop_auto_buy_probability_matrix.csv')\n",
    "rev_stop_probability_matrix = pd.read_csv('rev_stop_probability_matrix.csv')\n",
    "\n",
    "def predict(x):   \n",
    "    if type(x) == dict:\n",
    "        dataframe = pd.DataFrame(x, index=[0], columns=['sku', 'national_inv', 'lead_time', 'in_transit_qty',\n",
    "                                                    'forecast_3_month', 'forecast_6_month', 'forecast_9_month',\n",
    "                                                    'sales_1_month', 'sales_3_month', 'sales_6_month', 'sales_9_month',\n",
    "                                                    'min_bank', 'potential_issue', 'pieces_past_due', 'perf_6_month_avg',\n",
    "                                                    'perf_12_month_avg', 'local_bo_qty', 'deck_risk', 'oe_constraint',\n",
    "                                                    'ppap_risk', 'stop_auto_buy', 'rev_stop'])\n",
    "    else:\n",
    "        dataframe = x\n",
    "    \n",
    "    dataframe = dataframe.drop('sku', axis=1) #dropping sku column\n",
    "    \n",
    "    if dataframe.iloc[-1].isna().all() == True:\n",
    "        dataframe = dataframe[:-1] #removing last row as there are NaN values\n",
    "\n",
    "    dataframe = dataframe.fillna(lead_time_mean) #mean imputation\n",
    "    dataframe.replace({'Yes': 1, 'No': 0}, inplace=True) #converting categorical features into binary features\n",
    "    \n",
    "    #adding binary_pieces_past_due\n",
    "    conditions = [dataframe['pieces_past_due'] == 0, dataframe['pieces_past_due'] > 0]\n",
    "    values = [0, 1]\n",
    "    dataframe['binary_pieces_past_due'] = np.select(conditions, values)\n",
    "    \n",
    "    #adding binary_local_bo_qty\n",
    "    conditions = [dataframe['local_bo_qty'] == 0, dataframe['local_bo_qty'] > 0]\n",
    "    values = [0, 1]\n",
    "    dataframe['binary_local_bo_qty'] = np.select(conditions, values)\n",
    "    \n",
    "    #imputing all categorical features\n",
    "    conditions_pt = [dataframe['potential_issue'] == 0, dataframe['potential_issue'] == 1]\n",
    "    values_pt = [potential_issue_probability_matrix['No'][0], potential_issue_probability_matrix['No'][1]]\n",
    "    dataframe['potential_issue'] = np.select(conditions_pt, values_pt)\n",
    "\n",
    "    conditions_dr = [dataframe['deck_risk'] == 0, dataframe['deck_risk'] == 1]\n",
    "    values_dr = [deck_risk_probability_matrix['No'][0], deck_risk_probability_matrix['No'][1]]\n",
    "    dataframe['deck_risk'] = np.select(conditions_dr, values_dr)\n",
    "\n",
    "    conditions_oe = [dataframe['oe_constraint'] == 0, dataframe['oe_constraint'] == 1]\n",
    "    values_oe = [oe_constraint_probability_matrix['No'][0], oe_constraint_probability_matrix['No'][1]]\n",
    "    dataframe['oe_constraint'] = np.select(conditions_oe, values_oe)\n",
    "\n",
    "    conditions_pp = [dataframe['ppap_risk'] == 0, dataframe['ppap_risk'] == 1]\n",
    "    values_pp = [ppap_risk_probability_matrix['No'][0], ppap_risk_probability_matrix['No'][1]]\n",
    "    dataframe['ppap_risk'] = np.select(conditions_pp, values_pp)\n",
    "\n",
    "    conditions_stp = [dataframe['stop_auto_buy'] == 0, dataframe['stop_auto_buy'] == 1]\n",
    "    values_stp = [stop_auto_buy_probability_matrix['No'][0], stop_auto_buy_probability_matrix['No'][1]]\n",
    "    dataframe['stop_auto_buy'] = np.select(conditions_stp, values_stp)\n",
    "\n",
    "    conditions_rev = [dataframe['rev_stop'] == 0, dataframe['rev_stop'] == 1]\n",
    "    values_rev = [rev_stop_probability_matrix['No'][0], rev_stop_probability_matrix['No'][1]]\n",
    "    dataframe['rev_stop'] = np.select(conditions_rev, values_rev)\n",
    "\n",
    "    filename = 'best_model_forest.h5'\n",
    "    best_model = pickle.load(open(filename, 'rb'))\n",
    "    predictions = best_model.predict(dataframe)\n",
    "    if len(predictions) == 1:\n",
    "        predictions = int(predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32f8d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from streamlit.proto.Markdown_pb2 import Markdown\n",
    "import streamlit.components.v1 as components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f518655d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'predict' from 'inference' (D:\\Program files\\Anaconda3\\lib\\site-packages\\inference\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RAMESH~1\\AppData\\Local\\Temp/ipykernel_19552/3321143538.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0minference\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'predict' from 'inference' (D:\\Program files\\Anaconda3\\lib\\site-packages\\inference\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import inference\n",
    "from inference import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7830e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 22:35:04.911 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run D:\\Program files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.title('Prediction of Backorders in Inventory Management')\n",
    "st.header('A Random Forest model trained with a balanced subsample class weight')\n",
    "st.markdown('created by: **Pratyush Mohit**')\n",
    "st.subheader(\"Upload a csv file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db959567",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_file = st.file_uploader(\"Choose a file...\", type=['csv'])\n",
    "if uploaded_file is not None:\n",
    "   dataframe = pd.read_csv(uploaded_file)\n",
    "   st.write(\"Loading...displaying first five rows\")\n",
    "   st.dataframe(data=dataframe.head(), width=730, height=200)\n",
    "   st.write(\"Predicting...\")\n",
    "   x = dataframe.drop('went_on_backorder', axis=1)\n",
    "   fig, ax = plt.subplots()\n",
    "   sns.heatmap(x.corr(), ax=ax)\n",
    "   plt.title('Correlation Matrix')\n",
    "   st.write(fig)\n",
    "   predictions = predict(x)\n",
    "   st.write(predictions)\n",
    "   st.write('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03a584ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_19552/570703633.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\RAMESH~1\\AppData\\Local\\Temp/ipykernel_19552/570703633.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    py -m streamlit run ipykernel_launcher.py [ARGUMENTS]\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88f8b6e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_19552/722235828.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\RAMESH~1\\AppData\\Local\\Temp/ipykernel_19552/722235828.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    streamlit run inference.py\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c676c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
